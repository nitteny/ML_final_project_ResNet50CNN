{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Shirley Cho\n#CECS 456 01\n#Machine Learning Final Project\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport os\nos.environ['PYTHONHASHSEED']=str(123)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random as python_random\nfrom numpy.random import seed\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport sklearn\nimport numpy as np\nnp.random.seed(123) \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\n# loads ResNet50 trained weights from internet for transfer learning\nfrom tensorflow.keras.applications.resnet50 import ResNet50\n\n\nimport pandas as pd \n\n# example of loading an image with the Keras API\nfrom keras.preprocessing.image import load_img\n\n#I used img_to_array to get my something x something x 3 for the pixels and features of the image\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import array_to_img\nimport matplotlib.image as mpimg\n\n#for data agumentation\n\n\n#I found random code to see if I could seed it to try to get same results each time\n#But I think the gpu and other things in CNN make it so it will still be a bit random\nseed(123)\nnp.random.seed(123) \npython_random.seed(123)\ntf.random.set_seed(123)\n\n\n\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n#Code I found to try and make sure everything is seeded so that it's not very random each run? xD\ndef reset_seeds():\n    np.random.seed(123) \n    python_random.seed(123)\n    tf.random.set_seed(123)\n    os.environ[\"PYTHONHASHSEED\"] = str(123)\n    \n\n#File paths to extract the images - they get passed into getImageArrays(name) function as 'name' argument to return an array of \n#150x150x3\nfileNameTrainN = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/train/NORMAL'\nfileNameTrainP = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/train/PNEUMONIA'\nfileNameValidN = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/NORMAL'\nfileNameValidP = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA'\nfileNameTestN ='/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/test/NORMAL'\nfileNameTestP = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/test/PNEUMONIA'\n\n#Function that takes the file path as an argument, finds all the files in that file path (which in this case are all images of chest xray)\n#Then it will turn those images to arrays of their pixels using img_to_array() function.\n#I also resize them to 150x150x3 because the RAM cannot handle how big these pictures are\n#I think resizing may make us lose data a bit. I'm not sure how big an influence that is though\n#Then I return an array of all the image files for this specific path\ndef getImageArrays(name):\n    array = []\n    for dirname, _, filenames in os.walk(name):\n        for filename in filenames:\n            if '.DS_Store' not in filename:\n                temp = (img_to_array(load_img(os.path.join(dirname, filename)))/255.);\n                resized = cv2.resize(temp, (150,150))\n                array.append(resized)\n            \n    array = np.array(array)\n    return array\n \n\n\n\n#Assign the arrays of the image data\ntrainNormX1 = getImageArrays(fileNameTrainN)\ntrainPneX = getImageArrays(fileNameTrainP)\nvalNormX = getImageArrays(fileNameValidN)\nvalPneX = getImageArrays(fileNameValidP)\ntestNormX = getImageArrays(fileNameTestN)\ntestPneX = getImageArrays(fileNameTestP)\n\n#y-label. 1 for pneumonia and 0 for normal lungs\npne = 1\nnorm = 0\n\n#Make training normal lung data larger by duplicating it x3 here (I just concatenated same data over and over again)\ntrainNormX = np.concatenate((trainNormX1,trainNormX1),axis=0)\ntrainNormX = np.concatenate((trainNormX1,trainNormX),axis=0)\n\n#y labels for each one (They are just an array of all 1's or 0's)\ntrainNormY = np.full(trainNormX.shape[0],norm)\ntrainPneY = np.full(trainPneX.shape[0],pne)\nvalNormY = np.full(valNormX.shape[0],norm)\nvalPneY = np.full(valPneX.shape[0],pne)\ntestNormY =  np.full(testNormX.shape[0],norm)\ntestPneY =  np.full(testPneX.shape[0],pne)\n\n#Printed out to test\nprint(\"trainNormX\")\nprint(trainNormX.shape)\nprint(trainNormX[0].shape) #150 x 150 x 3\nplt.imshow(trainNormX[0])\nplt.show()\n\n#Image data agumentation modes\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True)\n\n#Get the resnet50 model from online and get them already trained weights on imagenet. The input shape should be our image input's shape\nres_model = tf.keras.applications.ResNet50(include_top = False,\n                                    weights=\"imagenet\",\n                                    input_shape = (150,150,3))\n\n#res_model.trainable = False\n\n#Set most of the layers as false to freeze them. I only set the last 4 layers as trainable to finetune\nfor layer in res_model.layers[:171]:\n    layer.trainable = False\n\n#printed out to check if they were actually set to False\nfor i, layer in enumerate(res_model.layers):\n    print(i,layer.name,\"-\",layer.trainable)\n    \nreset_seeds()    \n\n#Building the model\ncnn = tf.keras.models.Sequential() #initializing the CNN\n\n#Added the resNet50 model that is frozen as part of our model\ncnn.add(res_model)\n\n#pooling later to decrease size\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2)) #Pooling \ncnn.add(tf.keras.layers.Flatten()) #Flaterning\ncnn.add(tf.keras.layers.Dense(units=4096, activation=\"relu\"))#Full Connection1\ntf.keras.layers.Dropout(0.2)\ncnn.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))#Full Connection2\ntf.keras.layers.Dropout(0.2)\ncnn.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))#Full Connection3\ncnn.add(tf.keras.layers.Dense(units= 2, activation= \"softmax\"))#Output layer #I tried sigmoid with binary_crossentropy and it was much worse\n\n#Use Adam with a learning rate of 10^-5 - lower learning rate to see if I could get better optimization ; more specific learning even if slower\nopt = keras.optimizers.Adam(learning_rate=0.00001)\ncnn.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=opt,\n              metrics=[\"accuracy\"])\n\nres_model.summary()\ncnn.summary()\n\n#Splitting the training and pneumonia for validation \n#(I split seperately to make sure that my validation had almost 50/50 of penumonia/normal instead of completely random)\nX_trainN, X_valN, y_trainN, y_valN = train_test_split(trainNormX, trainNormY, test_size=0.1, random_state = 123) #Split train and val for normal lungs\nX_trainP, X_valP, y_trainP, y_valP = train_test_split(trainPneX, trainPneY, test_size=0.1, random_state = 123) #Split train and val for penumonia lungs\n\n#I'm adding normal and penumonia together for training\nX = np.concatenate((X_trainN,X_trainP),axis=0)\ny = np.concatenate((y_trainN,y_trainP),axis=0)\n\n#I'm adding the extra 16 pneumonia/normal images from the given validation test set into training because why not\nX = np.concatenate((X,valPneX),axis=0)\ny = np.concatenate((y,valPneY),axis=0)\n\nprint(X.shape)\n#print(X)\nprint(\"----------\")\nprint(y.shape)\n# print(y)\n\n#I'm adding normal and pneumonia together for validation \nX_val = np.concatenate((X_valN,X_valP),axis=0)\ny_val = np.concatenate((y_valN,y_valP),axis=0)\n\ntrain_datagen.fit(X)\n\n#it = train_datagen.flow(X, y,batch_size = 10)\n\n# generate image of augmented image to see what it looks like\n#batch = it.next()\nfor X_batch, y_batch in train_datagen.flow(X, y, batch_size=10):\n    # Show 9 images\n    for i in range(0, 10):\n        plt.imshow(X_batch[i].reshape(150, 150, 3))\n    plt.show()\n    break\n    \n    # show the plot\n# show the figure\n    \n\n#TRAINING MODEL on the split training set with validation set; shuffle set to True because I never shuffled any of my data\n#I chose to train 5 epochs on the regular not data agumented data\ncnn.fit(X, y, batch_size = 80, \n                            epochs=5, \n                            validation_data =(X_val,y_val),\n                            shuffle = True\n                           )\n#Then I chose to train the model on 11 epochs of the agumented data to avoid over fitting \n#It should be agumented on the fly\n#Validation set is the same and not agumented\nhistory = cnn.fit(train_datagen.flow(X, y, batch_size = 80), steps_per_epoch=X.shape[0]//80,\n                            epochs=20, \n                            validation_data =(X_val,y_val),\n                            validation_steps = X_val.shape[0]//80,\n                            shuffle = True)\n\n#Creating the test set after training the model - Just used the given test set from the files\n#Just adding the normal and pneumonia togerther though\nX_test = np.concatenate((testNormX,testPneX),axis = 0)\ny_test = np.concatenate((testNormY,testPneY),axis = 0)\n\nprint(\"Evaluation on test set\")\nscore = cnn.evaluate(X_test, y_test)\n\n\nX_new = testNormX[:10]\ny_proba = cnn.predict(X_new)\nprint(y_proba)\nprint(y_proba.round(2))\n\ny_pred = np.argmax(cnn.predict(X_new), axis=-1)\nprint(y_pred)\n\nX_new_ = testPneX[:10]\ny_proba_ = cnn.predict(X_new_)\nprint(y_proba_)\nprint(y_proba_.round(2))\n\ny_pred_ = np.argmax(cnn.predict(X_new_), axis=-1)\nprint(y_pred_)\n\n\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nprint(\"--TEST:--\")\n#Test set confusion matrix and results\ny_pred = np.argmax(cnn.predict(X_test), axis=-1)\nprint(\"-y_pred-\")\nprint(y_pred)\nconfusionMatrix = confusion_matrix(y_test,y_pred)\n\nprint(\"Confusion matrix:\")\nprint(confusionMatrix)\nlabels = [\"Normal\",\"Pneumonia\"]\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=labels)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()\n\nacc = accuracy_score(y_test,y_pred)\nprint(\"Accuracy score:\")\nprint(acc)\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\n\n# Print f1, precision, and recall scores\nprint(\"precision score: \")\nprint(precision_score(y_test, y_pred , average=\"macro\"))\nprint(\"recall score: \")\nprint(recall_score(y_test, y_pred , average=\"macro\"))\nprint(\"f1 score: \")\nprint(f1_score(y_test, y_pred , average=\"macro\"))\n\n\nprint(\"--VAL:--\")\n#Validation Set information results and confusion matrix\ny_pred = np.argmax(cnn.predict(X_val), axis=-1)\nconfusionMatrix = confusion_matrix(y_val,y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=labels)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()\n\nacc = accuracy_score(y_val,y_pred)\nprint(\"Accuracy score:\")\nprint(acc)\n\n\n# Print f1, precision, and recall scores\nprint(\"precision score: \")\nprint(precision_score(y_val, y_pred , average=\"macro\"))\nprint(\"recall score: \")\nprint(recall_score(y_val, y_pred , average=\"macro\"))\nprint(\"f1 score: \")\nprint(f1_score(y_val, y_pred , average=\"macro\"))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-30T06:08:06.766218Z","iopub.execute_input":"2022-04-30T06:08:06.766697Z","iopub.status.idle":"2022-04-30T06:24:20.873262Z","shell.execute_reply.started":"2022-04-30T06:08:06.766604Z","shell.execute_reply":"2022-04-30T06:24:20.871410Z"},"trusted":true},"execution_count":null,"outputs":[]}]}